""" 

This file is intended to be a catch all for misc functions

"""

from typing import Optional, List
import cv2
import os

from data import ExperienceReplay
from Models.base import TransitionModel
from env import BaseEnv

import numpy as np
import torch


def gather_data(
    env: BaseEnv, 
    memory: ExperienceReplay, 
    n_trajectories: int = 5
) -> None:
    """
    Gather N trajectories using random actions and add the transitions to the
    experience replay memory.
    """

    for _ in range(n_trajectories):
        state = env.reset()
        done = False
        while not done:
            action = env.sample_random_action()
            next_state, reward, done, info = env.step(action)
            memory.append(next_state, action, reward, done)

    env.close()


def compute_loss(
    transition_model: TransitionModel,
    memory: ExperienceReplay,
    kl_clip: torch.Tensor,
    global_prior_means: torch.Tensor,
    global_prior_stddvs: torch.Tensor,
    train_config: dict,
    model_type: str
) -> List[torch.Tensor]:
    """
    Shared Loss function for all models. All models have the following 3 methods:
        model.kl_loss
        model.obervation_loss
        model.reward_loss
    """
    loss = 0

    # Sample batch_size sequences of length at random from the replay buffer.
    # Our sampled transitions are treated as starting from a random intial state at time 0.
    observations, actions, rewards, nonterminals = memory.sample(
        train_config["batch_size"], train_config["seq_length"]
    )

    # we start all models with an initial state and belief of 0's
    init_belief = torch.zeros(train_config["batch_size"], transition_model._belief_size).to(transition_model._device)
    init_state = torch.zeros(train_config["batch_size"], transition_model._state_size).to(transition_model._device)

    # encode the observations by passing them through the models encoder network.
    encoded_observations = transition_model.encode(observations)
    (
        beliefs,
        prior_states,
        prior_means,
        prior_stddvs,
        posterior_states,
        posterior_means,
        posterior_stddvs,
        reward_preds,
    ) = transition_model(
        init_state,
        actions[:-1],
        init_belief,
        encoded_observations[1:],
        nonterminals[:-1]
    )

    # The loss function is the sum of the reconstruction loss, the reward prediction loss, and the KL-divergence loss.
    kl_loss = transition_model.kl_loss(prior_means, prior_stddvs, posterior_means, posterior_stddvs, kl_clip)
    obs_loss = transition_model.observation_loss(posterior_states, beliefs, observations[1:])
    rew_loss = transition_model.reward_loss(reward_preds, rewards[:-1])
    loss = kl_loss + obs_loss + rew_loss

    # The Global KL divergence term is not applicable to the RNN based Transition model.
    if 0 < train_config["global_kl_beta"] and model_type != 'rnn':
        loss += train_config["global_kl_beta"] * transition_model.kl_loss(
            global_prior_means,
            global_prior_stddvs,
            posterior_means,
            posterior_stddvs,
            kl_clip,
        )

    return kl_loss, obs_loss, rew_loss, loss 


def write_video(frames: np.array, title: str, path=''):
    """
        Used to Visualize the trajectories generated by the model.
        frames will be a numpy array of dimenstion [horizon, color, width, height].
        
    """
    frames = np.stack(frames, axis=0)
    frames = np.multiply(frames, 255).clip(0, 255).astype(np.uint8)
    frames = frames.transpose(0, 2, 3, 1)[:, :, :, ::-1]

    # frames = np.multiply(np.stack(frames, axis=0).transpose(0, 2, 3, 1), 255).clip(0, 255).astype(np.uint8)[:, :, :, ::-1]  # VideoWrite expects H x W x C in BGR
    _, H, W, _ = frames.shape
    writer = cv2.VideoWriter(os.path.join(path, '%s.mp4' % title), cv2.VideoWriter_fourcc(*'mp4v'), 30., (W, H), True)
   
    for frame in frames:
        writer.write(frame)
    writer.release()


def update_belief_and_act(env, transition_model, belief, posterior_state, action, observation):
    # Infer belief over current state q(s_t|oâ‰¤t,a<t) from the history
    belief, _, _, _, posterior_state, _, _, _ = transition_model(posterior_state, action.unsqueeze(0), belief, transition_model.encode(observation).unsqueeze(dim=0))  # Action and observation need extra time dimension
    belief, posterior_state = belief.squeeze(dim=0), posterior_state.squeeze(dim=0)
    action = -1 + 2*np.random.rand()
    action = -1 + 2*torch.rand(1, env.action_size, device=transition_model._device)
    next_observation, reward, done, _ = env.step(action[0].cpu())
    return belief, posterior_state, action, next_observation, reward, done
