# this config file defines the necessary hyperparameters for traingin
# The SSM model. The primary difference is that we use a 
# global_KL_beta value of 1, rather than 0 for the RSSM and RNN models


project: "E6691 Final: Learning Latent Dynamics for Planning from Pixels"
authors: 
  - "Sam Fieldman"
  - "Trevor Gordon"


# Stochastic State Space Model set-up
# Only posses the Feed-Foward Probalistic transition model
ssm:
  activation: "ELU"
  embed_size: 1024
  hidden_dim: 400
  belief_size: 0
  state_size: 30

# Recurrent State Space Model set-up
# Only posses the RNN transition model
rnn:
  activation: "ELU"
  embed_size: 1024
  hidden_dim: 200
  belief_size: 200
  state_size: 0

# Recurrent State Space Model set-up
# Contains both SSM and RNN components
rssm:
  activation: "ELU"
  embed_size: 1024
  hidden_dim: 200
  belief_size: 200
  state_size: 30

# Memory buffer hyperparameters
memory:
  size: 1_000_000
  symbolic_env: False
  bit_depth: 5


# environments hyperparameters
env:
  symbolic_env: False
  max_episode_length: 1000
  action_repeat: 8 
  bit_depth: 5
 

# Training hyperparameters
train:
# total number of trajectories to collect.
  episodes: 1000
  seed_episodes: 5
  clip_grad_norm: 1_000
  learning_rate: 0.001
  lr_schedule: 0
  adam_epsilon: 0.0001
  batch_size: 50
  train_iters: 100
  seq_length: 50

  # weight parameters for KL-divergence and latent overshooting loss terms 
  kl_clip: 3.0
  global_kl_beta: 1
  overshooting_distance: 50
  overshooting_kl_beta: 1
  overshooting_reward_beta: 1


# Model Predictive Control Hyperparameters
mpc:
  planning_horizon: 12
  optimization_iters: 10
  candidates: 1000
  top_candidates: 100

# Model Predictive Control Hyperparameters Just for data collection
mpc_data_collection:
  planning_horizon: 12
  optimization_iters: 10
  candidates: 1000
  top_candidates: 100
  exploration_noise: 0.3

   
# Miscellanious parameters for data collection
# experiment tracking, and experiment set-up

# how frequently to test the model, the number of 
# trajectories to test with, and how often to checkpoint 
# our progress.
test_interval: 5
test_episodes: 10
checkpoint_interval: 25
updates_per_one_episode_collection: 100
